{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Ivan Dario Ovalle Benavides\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09b45cef14501d634174ac2db3301d22",
     "grade": false,
     "grade_id": "cell-ef4bc4b82edc1fe2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"https://hadoop.apache.org/docs/r1.2.1/images/hadoop-logo.jpg\">\n",
    "\n",
    "# Sistema de ficheros HDFS: configuración del entorno de la asignatura\n",
    "\n",
    "Como ya se ha visto en la parte de teoría el sistema de archivos Hadoop (HDFS) es una parte fundamental del entorno Big Data de Apache Hadoop. En esta práctica primeramente exploraremos cómo podemos interactuar desde la línea de comandos con el sistema de ficheros HDFS. El primer paso es abrir un terminal desde el Jupyter o bien ejecutar los comandos mediante el comando \"!\" dentro del Jupyter.\n",
    "\n",
    "Los comandos que se pueden enviar al sistema de archivos son muy similares a las de bash en entornos Linux.\n",
    "\n",
    "Para listar los archivos del directorio raíz del HDFS utilizaremos el comando *ls*. \n",
    "\n",
    "`hdfs dfs -ls /`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dafc32737acb11e3f0336e7f23210bac",
     "grade": false,
     "grade_id": "cell-e76195505befed00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\r\n",
      "drwxr-xr-x   - asolerib   supergroup          0 2020-01-16 22:12 /CFCC\r\n",
      "drwxr-xr-x   - asolerib   supergroup          0 2021-05-13 17:37 /aula_B0.476\r\n",
      "drwxr-xr-x   - asolerib   supergroup          0 2019-10-28 11:09 /aula_B2.585\r\n",
      "drwxr-xr-x   - asolerib   supergroup          0 2019-09-25 22:10 /aula_M2.858\r\n",
      "drwxr-xr-x   - hbase      hbase               0 2021-09-13 14:37 /hbase\r\n",
      "drwxr-xr-x   - aperezgari supergroup          0 2021-08-05 12:58 /home\r\n",
      "drwxrwxr-x   - solr       solr                0 2019-07-23 15:49 /solr\r\n",
      "drwxrwxrwt   - hdfs       supergroup          0 2021-05-25 22:52 /tmp\r\n",
      "drwxrwxr-x   - hdfs       supergroup          0 2021-09-27 10:19 /user\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a735c329e1bad1c42da73a74f620c3d",
     "grade": false,
     "grade_id": "cell-5683abc1be5f6f21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "0. Ahora se pide listar los ficheros del directorio /aula_M2.858. (1 punto)\n",
    "\n",
    "Se pide rellenar la siguiente celda con el comando requerido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "049c68dc65f79ab5c8302fd40ed6f24a",
     "grade": true,
     "grade_id": "cell-6d6487d565295f3c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - asolerib supergroup          0 2020-09-21 22:02 /aula_M2.858/PECs\r\n",
      "drwxr-xr-x   - asolerib supergroup          0 2021-10-06 16:09 /aula_M2.858/data\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls  /aula_M2.858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1adbca1ea9307a6d3667580455059c8b",
     "grade": false,
     "grade_id": "cell-83daaea50a3a19b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Tal y como se verá a continuación los comandos que HDFS proporciona, permiten: listar, consultar, subir o bajar archivos desde el directorio local en HDFS. Como ya se ha podido ver el directorio `aula_M2.858` contiene una serie de subdirectorios donde se proporcionarán algunos archivos para hacer los siguientes ejercicios. A continuación se pide ejecutar una serie de operaciones que dejarán su directorio hdfs personal configurado para la realización de la asignatura. Puede encontrar el manual de los comandos del sistema HDFS en el siguiente link [HDFS Commands](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html).\n",
    "\n",
    "\n",
    "\n",
    "1. Se pide crear la carpeta \"data\" dentro del directorio HDFS /user/\\< loginestudiante \\>/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6ceb9764458c5bd5a8ef323452b55cd",
     "grade": true,
     "grade_id": "cell-06a1507ab382a0ed",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/ivanovalle/data': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/ivanovalle/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "993bb2b9d1d6f711a5d32c4eea1e8b3c",
     "grade": false,
     "grade_id": "cell-a00aa8ec1480f7fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Copiar al directorio \"data\" que acabáis de crear el fichero \"Folds5x2_pp.csv\" que se encuentra en el directorio HDFS /aula_M2.858/data/. Utilizad el comando `hdfs dfs -cp` para copiar el fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "813c28104de02ed88814cad57213f881",
     "grade": true,
     "grade_id": "cell-91119f3bf356c1df",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: `/user/ivanovalle/data/Folds5x2_pp.csv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cp /aula_M2.858/data/Folds5x2_pp.csv /user/ivanovalle/data/Folds5x2_pp.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4c5d4777d4e37ea59c720cb84796d7a",
     "grade": false,
     "grade_id": "cell-7e10129877bb8963",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Copiar las 10 primeras líneas del fichero LoremIpsum.txt que se encuentra en el directorio HDFS /aula_M2.858/data/ al fichero LoremIpsum_10.txt al directorio HDFS /user/\\< loginestudiante \\>/data. Os serán de utilidad los comandos `hdfs dfs -cat` i `hdfs dfs -put` a parte del comando clásico de bash `head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07262a5703f4f0ff2f0450fa5c933700",
     "grade": true,
     "grade_id": "cell-7ab813bca849cf49",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n",
      "put: `/user/ivanovalle/data/LoremIpsum_10.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "#START <FILL IN>\n",
    "!hdfs dfs -cat /aula_M2.858/data/LoremIpsum.txt | head -n 10 > LoremIpsum_10.txt\n",
    "!hdfs dfs -put LoremIpsum_10.txt /user/ivanovalle/data/LoremIpsum_10.txt\n",
    "#END <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f15bccd4871b97b759ffe98ca6d1599",
     "grade": false,
     "grade_id": "cell-742b82373f54b03b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A parte de los comandos que ya habéis visto orientados a que cada usuario pueda gestionar el sistema de ficheros HDFS, también es posible explorar la organización interna de los diferentes ficheros existentes en los directorios HDFS.\n",
    "\n",
    "4. Se pide que mediante el comando `hdfs fsck` determinar y describir como esta estructurado el fichero _/aula_M2.858/data/LoremIpsum.txt_ dentro del sistema HDFS. En concreto se pide mostrar como está subdividido en bloques, en que nodo están los bloques y si estos están replicados. Podéis consultar como utilizar el comando `hdfs fsck` en la web oficial de referéncia -> [HDFS Commands](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22147f26c33ab56540699672bf9295b9",
     "grade": true,
     "grade_id": "cell-b4b52072400d78a8",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://eimtcld.uoc.edu:9870/fsck?ugi=ivanovalle&files=1&blocks=1&racks=1&path=%2Faula_M2.858%2Fdata%2FLoremIpsum.txt\r\n",
      "FSCK started by ivanovalle (auth:SIMPLE) from /213.73.35.119 for path /aula_M2.858/data/LoremIpsum.txt at Sun Oct 10 06:55:03 CEST 2021\r\n",
      "/aula_M2.858/data/LoremIpsum.txt 203431368 bytes, replicated: replication=3, 2 block(s):  OK\r\n",
      "0. BP-2074018746-213.73.35.119-1563889676427:blk_1073781405_40591 len=134217728 Live_repl=3  [/default/10.20.30.3:9866, /default/10.20.30.2:9866, /default/213.73.35.119:9866]\r\n",
      "1. BP-2074018746-213.73.35.119-1563889676427:blk_1073781406_40592 len=69213640 Live_repl=3  [/default/10.20.30.3:9866, /default/10.20.30.2:9866, /default/213.73.35.119:9866]\r\n",
      "\r\n",
      "\r\n",
      "Status: HEALTHY\r\n",
      " Number of data-nodes:\t3\r\n",
      " Number of racks:\t\t1\r\n",
      " Total dirs:\t\t\t0\r\n",
      " Total symlinks:\t\t0\r\n",
      "\r\n",
      "Replicated Blocks:\r\n",
      " Total size:\t203431368 B\r\n",
      " Total files:\t1\r\n",
      " Total blocks (validated):\t2 (avg. block size 101715684 B)\r\n",
      " Minimally replicated blocks:\t2 (100.0 %)\r\n",
      " Over-replicated blocks:\t0 (0.0 %)\r\n",
      " Under-replicated blocks:\t0 (0.0 %)\r\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\r\n",
      " Default replication factor:\t3\r\n",
      " Average block replication:\t3.0\r\n",
      " Missing blocks:\t\t0\r\n",
      " Corrupt blocks:\t\t0\r\n",
      " Missing replicas:\t\t0 (0.0 %)\r\n",
      " Blocks queued for replication:\t0\r\n",
      "\r\n",
      "Erasure Coded Block Groups:\r\n",
      " Total size:\t0 B\r\n",
      " Total files:\t0\r\n",
      " Total block groups (validated):\t0\r\n",
      " Minimally erasure-coded block groups:\t0\r\n",
      " Over-erasure-coded block groups:\t0\r\n",
      " Under-erasure-coded block groups:\t0\r\n",
      " Unsatisfactory placement block groups:\t0\r\n",
      " Average block group size:\t0.0\r\n",
      " Missing block groups:\t\t0\r\n",
      " Corrupt block groups:\t\t0\r\n",
      " Missing internal blocks:\t0\r\n",
      " Blocks queued for replication:\t0\r\n",
      "FSCK ended at Sun Oct 10 06:55:03 CEST 2021 in 1 milliseconds\r\n",
      "\r\n",
      "\r\n",
      "The filesystem under path '/aula_M2.858/data/LoremIpsum.txt' is HEALTHY\r\n"
     ]
    }
   ],
   "source": [
    "#Run the command to obtain the required information regarding the structure of file `/aula_M2.858/data/LoremIpsum.txt`\n",
    "#START <FILL IN> \n",
    "!hdfs fsck /aula_M2.858/data/LoremIpsum.txt -files -blocks -racks\n",
    "#END <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f651d1ee0f9979099e1c4c01f5b8e6f",
     "grade": true,
     "grade_id": "cell-020e0d30314844a9",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Describe in words the questions regarding the file `/aula_M2.858/data/LoremIpsum.txt`\n",
    "\n",
    "# El archivo se dividio en 2 bloques, se crearon 3 nodos de datos y 3 replicas.\n",
    "\n",
    "# Imagino que se vería así:\n",
    "# Archivo = [B1,B2]\n",
    "\n",
    "# Cloudera01:9866 (213.73.35.119:9866) = [B1,B2]\n",
    "# Cloudera02:9866 (10.20.30.2:9866) = [B1,B2]\n",
    "# Cloudera03:9866 (10.20.30.3:9866) = [B1,B2]\n",
    "\n",
    "# En donde los cloudera serían los nodos de datos, B1 y B2 los bloques, y hay 3 replicas de estos bloques.\n",
    "\n",
    "\n",
    "#END <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62df82cd7bef5be6e21596ab0862caa3",
     "grade": false,
     "grade_id": "cell-25c6c7af099b6a2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. Como sabeis los diferentes ficheros HDFS están particionados en bloques. Podeis consultar la configuración mediante el comando: `hdfs getconf -confKey dfs.blocksize`. ¿Cuantos MB hay por bloque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a148b9cbfa26bd3be32f6647f00944f",
     "grade": true,
     "grade_id": "cell-57391a012900edc9",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728\r\n"
     ]
    }
   ],
   "source": [
    "#START <FILL IN>\n",
    "\n",
    "!hdfs getconf -confKey dfs.blocksize\n",
    "\n",
    "# 134217728, 128 MB\n",
    "\n",
    "#END <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efd743701b6ae1bf0af0a1fb6b47dd87",
     "grade": false,
     "grade_id": "cell-74a9ee63d84e17b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Se pide explorar la estructura del sistema HDFS mediante su WebUI. Para ello primeramente debemos creare un túnel ssh que nos dé acceso al puerto 9870 (sobre el que está corriendo la Web UI de HDFS). A los alumnos que tengáis un sistema basado en Linux, podéis hacerlo de manera muy simple mediante el siguiente comando:\n",
    "\n",
    "```python \n",
    "ssh -p55000  -L 9870:eimtcld.uoc.edu:9870 asolerib@eimtcld.uoc.edu\n",
    "        \n",
    "```\n",
    "Una vez ejecutado el comando y logeado correctamente podéis acceder a la Web UI de HDFS en la dirección:\n",
    "\n",
    "```Python\n",
    "http://localhost:9870/dfshealth.html#tab-overview\n",
    "```\n",
    "\n",
    "> Nota: a los alumnos que solo dispongan de un sistema Windows, adjuntamos una pequeña descripción para crear el túnel ssh mediante la aplicación Putty.\n",
    "\n",
    "Esta pregunta es de texto libre y se pide que se describa a continuación qué información podéis obtener explorando la Web UI, número de DataNodes, localización de estos y su estado, factores de replicación, errores si los hay, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7e1a2edd9164f7689260b09ea141a5c",
     "grade": true,
     "grade_id": "cell-064c7ff9aeb78988",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Hay 3 DataNodes:\n",
    "# Cloudera01:9866 (213.73.35.119:9866)\n",
    "# Cloudera02:9866 (10.20.30.2:9866)\n",
    "# Cloudera03:9866 (10.20.30.3:9866)\n",
    "\n",
    "# Todos en OK (In service)\n",
    "\n",
    "# Configured Capacity 2.18 TB, DFS Used 880.81 (39.48%)\n",
    "\n",
    "# Default replication factor: 3\n",
    "\n",
    "# Sin errores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
